{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fe74d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root to the Python path\n",
    "package_root = os.path.abspath(os.path.join(os.getcwd(), \"../\"))\n",
    "print(f\"Adding package root to sys.path: {package_root}\")\n",
    "if package_root not in sys.path:\n",
    "\tsys.path.append(package_root)\n",
    "\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "print(f\"Current notebook directory: {notebook_dir}\")\n",
    "# change to the directory to the root of the project\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "print(f\"Project root: {project_root}\")\n",
    "os.chdir(project_root)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0aa6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.environ[\"LANGCHAIN_TRACING_V2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379a5c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from lets_talk.utils import get_message_text\n",
    "import importlib\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d03278c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display marked down\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "#printmd(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95246cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "config = RunnableConfig(\n",
    "        configurable={\"user_id\": \"abc\",\"thread_id\":\"abc-1\" }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3259b5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lets_talk import agent\n",
    "importlib.reload(agent)\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "uncompiled_graph = agent.build_graph()\n",
    "graph =  uncompiled_graph.compile(checkpointer=checkpointer)\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871a9635",
   "metadata": {},
   "outputs": [],
   "source": [
    "response =  await graph.ainvoke({\"messages\":[HumanMessage(content=\"I'm Isa, who are you?\")] },config=config)\n",
    "printmd(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9dd31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response =  await graph.ainvoke({\"messages\":[HumanMessage(content=\"Who am I?\")] },config=config)\n",
    "printmd(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a06d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363845a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response =  await graph.ainvoke({\"messages\":[HumanMessage(content=\"I don't like you?\")] },config=config)\n",
    "printmd(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109e7045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the config dictionary to include recursion_limit\n",
    "config_with_recursion = {**config, \"recursion_limit\": 10}\n",
    "\n",
    "query = \"my email is isa@thedataguy.pro, send a message to thedataguy that I like your blog.\"\n",
    "\n",
    "response = await graph.ainvoke({\"question\": query,\"messages\":[HumanMessage(content=query)]}, config=config_with_recursion)\n",
    "printmd(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f422a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42379ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await graph.ainvoke({\"question\": \"What is current time in CDT?\"},config=config)\n",
    "\n",
    "printmd(response[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf5a457",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beddb990",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await graph.ainvoke({\"question\": \"What is current time in CDT?\"},config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90b49f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for chunk,_ in graph.astream({\"messages\":[HumanMessage(content=\"Give me latest blog posts\")]}, stream_mode=\"messages\", config=config):\n",
    "    print(chunk.content,end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738447c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"TOGETHER_API_KEY\"] = getpass.getpass(\"Enter your Together API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11494eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "config = RunnableConfig(\n",
    "        configurable={\"user_id\": \"abc\",\"thread_id\":\"abc-1\" }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae092dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lets_talk.tools as lets_talk_tools\n",
    "\n",
    "importlib.reload(lets_talk_tools)\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.tools import tool\n",
    "from lets_talk import rag\n",
    "from lets_talk.utils import format_docs\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_community.tools.requests.tool import RequestsGetTool\n",
    "from langchain_community.utilities.requests import TextRequestsWrapper\n",
    "\n",
    "\n",
    "@tool \n",
    "def retrive_documents(query: str) -> str:\n",
    "    \"\"\"Retrieve relevant documents from the knowledge base to answer user questions.\n",
    "    \n",
    "    Use this tool when you need to search for specific information, facts, or content\n",
    "    that may be in the document collection. Provide a clear search query related to\n",
    "    what information you need to find.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query to find relevant documents\n",
    "        \n",
    "    Returns:\n",
    "        Formatted text containing the retrieved document content\n",
    "    \"\"\"\n",
    "    docs = rag.retriever.invoke(query) \n",
    "    return format_docs(docs)\n",
    "\n",
    "\n",
    "prompt = \"\"\"\n",
    "You are TheDataGuy Chat, a specialized assistant powered by content from Muhammad Afzaal (TheDataGuy)'s blog at thedataguy.pro. You are expert in data science, AI evaluation, RAG systems, research agents, and metric-driven development.\n",
    "\n",
    "## Your Purpose\n",
    "You provide practical, insightful responses to queries about topics covered in TheDataGuy's blog posts, including:\n",
    "- RAGAS and evaluation frameworks for LLM applications\n",
    "- RAG (Retrieval-Augmented Generation) systems and their implementation\n",
    "- Building and evaluating AI research agents\n",
    "- Metric-Driven Development for technology projects\n",
    "- Data strategy and its importance for business success\n",
    "- Technical concepts in AI, LLM applications, and data science\n",
    "\n",
    "## Tools Usage\n",
    "- Always use the 'retrive_documents' tool when you need to search for information from blog posts or articles\n",
    "- Use this tool before answering questions about specific content, examples, or details from TheDataGuy's blog\n",
    "- When using the retrieval tool, provide clear and specific search queries related to the user's question\n",
    "\n",
    "## Response Guidelines\n",
    "1. Generate clear, concise responses in markdown format\n",
    "2. Include relevant links to blog posts to help users find more information\n",
    "3. For code examples, use appropriate syntax highlighting\n",
    "4. When practical, provide actionable steps or implementations\n",
    "5. Maintain a helpful, informative tone consistent with TheDataGuy's writing style\n",
    "6. When providing links, use the URL format from the context: [title or description](URL)\n",
    "7. When discussing a series of blog posts, mention related posts when appropriate\n",
    "8. When faced with rude queries or negative comments, respond with graceful, upbeat positivity and redirect the conversation toward helpful topics\n",
    "\n",
    "## Special Cases\n",
    "- If the context is unrelated to the query, respond with \"I don't know\" and suggest relevant topics that are covered in the blog\n",
    "- If asked about topics beyond the blog's scope, politely explain your focus areas and suggest checking thedataguy.pro for the latest content\n",
    "- Use real-world examples to illustrate complex concepts, similar to those in the blog posts\n",
    "- For rude or impolite queries, maintain a positive and professional tone, never responding with rudeness, and gently steer the conversation back to productive topics\n",
    "\n",
    "Remember, your goal is to help users understand TheDataGuy's insights and apply them to their own projects and challenges, always maintaining a helpful and positive attitude regardless of how the query is phrased.\n",
    "\"\"\"\n",
    "\n",
    "requests_tool = RequestsGetTool(\n",
    "    requests_wrapper=TextRequestsWrapper(headers={}),\n",
    "    allow_dangerous_requests=True,\n",
    "    description=\"Use this tool to make HTTP GET requests to retrieve information from the web. Provide a valid URL to fetch data.\",\n",
    ")\n",
    "\n",
    "tools =[lets_talk_tools.RSSFeedTool(), lets_talk_tools.get_current_datetime,retrive_documents,requests_tool]\n",
    "\n",
    "model_name = \"openai:gpt-4o-mini\"\n",
    "#model_name = \"together:meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\"\n",
    "#model_name = \"ollama:cogito\"\n",
    "#model_name = \"together:meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\"\n",
    "model = init_chat_model(model_name, temperature=0.0)\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=tools,\n",
    "    prompt=prompt,\n",
    "    version=\"v2\",\n",
    "    checkpointer=checkpointer\n",
    "    \n",
    ")\n",
    "\n",
    "# Run the agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7811d314",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Explain thedataguy data value chain?\"}]},config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdd9d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"I'm Isa, who are you?\"}]},config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8196abeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is my name?\"}]},config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d68b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is thedataguy's data value chain?\"}]},config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d47808",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Who are you?\"}]},config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba006fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Who is thedataguy?\"}]},config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc599b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is latest blog post?\"}]},config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9078cda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Explain Truth is Cold by fetching contents\"}]},config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1c2112",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Summarize https://thedataguy.pro/blog/2025/05/truth-is-cold\"}]},config=config\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f7aaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = agent.get_state(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5fa3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in state.values[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56bb97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Explain lastest post\"}]},config=config\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
