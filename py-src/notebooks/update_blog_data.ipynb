{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1a955e7",
   "metadata": {},
   "source": [
    "# Update Blog Data\n",
    "\n",
    "This notebook demonstrates how to update the blog data and vector store when new blog posts are published. It uses the utility functions from `utils_data_loading.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec048b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root to the Python path\n",
    "package_root = os.path.abspath(os.path.join(os.getcwd(), \"../\"))\n",
    "print(f\"Adding package root to sys.path: {package_root}\")\n",
    "if package_root not in sys.path:\n",
    "\tsys.path.append(package_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7a9f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_dir = os.getcwd()\n",
    "print(f\"Current notebook directory: {notebook_dir}\")\n",
    "# change to the directory to the root of the project\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "print(f\"Project root: {project_root}\")\n",
    "os.chdir(project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc19ab4c",
   "metadata": {},
   "source": [
    "## Update Blog Data Process\n",
    "\n",
    "This process will:\n",
    "1. Load existing blog posts\n",
    "2. Process and update metadata\n",
    "3. Create or update vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9e4191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lets_talk.config import DATA_DIR,WEB_URLS, BASE_URL, BLOG_BASE_URL,DATA_DIR_PATTERN, QDRANT_COLLECTION,QDRANT_URL\n",
    "data_dir = DATA_DIR\n",
    "pattern = DATA_DIR_PATTERN \n",
    "base_url = BASE_URL\n",
    "blog_base_url = BLOG_BASE_URL\n",
    "web_urls = WEB_URLS\n",
    "#index_only_published_posts = True\n",
    "# print config values\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "print(f\"Data directory pattern: {pattern}\")\n",
    "print(f\"Base URL: {base_url}\")\n",
    "print(f\"Blog Base URL: {blog_base_url}\")\n",
    "print(f\"Web URLs: {web_urls}\")\n",
    "# Ensure the data directory exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d56f688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import lets_talk.utils.blog as  blog_utils\n",
    "docs = blog_utils.load_blog_posts(data_dir=data_dir,glob_pattern=pattern)\n",
    "#TODO: implement `index_only_published_posts` to filter out unpublished posts\n",
    "docs_with_data = blog_utils.update_document_metadata(docs,data_dir_prefix=data_dir+'/', base_url=base_url, blog_base_url=blog_base_url, remove_suffix=pattern)\n",
    "loader = WebBaseLoader(web_urls)\n",
    "web_docs = loader.load()\n",
    "all_docs = docs_with_data + web_docs\n",
    "print(f\"Total documents loaded: {len(all_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efb65de",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs_with_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9041da56",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9c8cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print urls\n",
    "for doc in all_docs:\n",
    "    print(f\"URL: {doc.metadata['url'] if 'url' in doc.metadata else doc.metadata['source']}\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35908dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Install matplotlib using uv\n",
    "%pip install matplotlib -q\n",
    "\n",
    "# Display document counts\n",
    "print(f\"Blog posts loaded: {len(docs_with_data)}\")\n",
    "print(f\"Web pages loaded: {len(web_docs)}\")\n",
    "print(f\"Total documents: {len(all_docs)}\")\n",
    "\n",
    "# Create a label counter for document sources\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract source types\n",
    "source_types = []\n",
    "for doc in all_docs:\n",
    "    source = doc.metadata.get('source', '')\n",
    "    if source.startswith(data_dir):\n",
    "        source_types.append('Blog Post')\n",
    "    else:\n",
    "        source_types.append('Web Page')\n",
    "\n",
    "# Count document types\n",
    "source_counter = Counter(source_types)\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "df_sources = pd.DataFrame(source_counter.items(), columns=['Source Type', 'Count'])\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(df_sources['Source Type'], df_sources['Count'], color=['#1f77b4', '#ff7f0e'])\n",
    "plt.title('Document Source Distribution')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "for i, count in enumerate(df_sources['Count']):\n",
    "    plt.text(i, count + 0.1, str(count), ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14c70dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split_docs = blog_utils.split_documents(all_docs, chunk_size=1000, chunk_overlap=200)\n",
    "#split_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2797a3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryStore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56b7269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of sub subclass of InMemoryStore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeec357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import init_embeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "embedding_model = init_embeddings(\"ollama:snowflake-arctic-embed2:latest\",base_url=\"http://host.docker.internal:11434\")\n",
    "semantic_chunker = SemanticChunker(\n",
    "    embedding_model, # type: ignore\n",
    "    breakpoint_threshold_type=\"percentile\"\n",
    ")\n",
    "semantic_documents = semantic_chunker.split_documents(all_docs)\n",
    "\n",
    "vector_store = QdrantVectorStore.from_documents(\n",
    "        semantic_documents,\n",
    "        embedding=embedding_model, #type: ignore\n",
    "        collection_name=QDRANT_COLLECTION,\n",
    "        url=QDRANT_URL,\n",
    "        prefer_grpc=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f342dc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037df68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from lets_talk.config import (LLM_MODEL, LLM_TEMPERATURE)\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.tools import tool\n",
    "from lets_talk.utils import format_docs\n",
    "\n",
    "model = init_chat_model(LLM_MODEL, temperature=LLM_TEMPERATURE)\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=retriever, llm=model\n",
    ")\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(all_docs)  # type: ignore\n",
    "retriever_list = [bm25_retriever,  multi_query_retriever]\n",
    "\n",
    "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=retriever_list, weights=[0.4, 0.6]  # Adjust weights as needed\n",
    ")\n",
    "\n",
    "@tool \n",
    "def retrive_documents(query: str) -> str:\n",
    "    \"\"\"Retrieve relevant documents from the knowledge base to answer user questions.\n",
    "    \n",
    "    Allways use this tool to search for specific information, facts, or content\n",
    "    that may be in the document collection. Provide a clear search query related to\n",
    "    what information to find.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query to find relevant documents\n",
    "        \n",
    "    Returns:\n",
    "        Formatted text containing the retrieved document content\n",
    "    \"\"\"\n",
    "    docs = ensemble_retriever.invoke(query) # type: ignore\n",
    "    return format_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe5560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from lets_talk.config import RSS_URL\n",
    "from lets_talk.tools.rss_feed_tool import RSSFeedTool\n",
    "from lets_talk.agent_v2 import prompt\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "tools =[retrive_documents]\n",
    "\n",
    "#RSS_URL = 'https://thedataguy.pro/rss.xml'  # Replace with your actual RSS feed URL\n",
    "if RSS_URL:\n",
    "    logging.info(f\"RSS URL is set to: {RSS_URL}\")\n",
    "    tools.append(RSSFeedTool(rss_url=RSS_URL))\n",
    "\n",
    "model = init_chat_model(model=\"openai:gpt-4o-mini\", temperature=LLM_TEMPERATURE)\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=tools,\n",
    "    prompt=prompt,\n",
    "    version=\"v2\",\n",
    ")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390ba14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "response = agent.invoke({\"messages\":[HumanMessage(content=\"What is the latest blog post about?\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788895c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd8242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from lets_talk.utils import get_message_text\n",
    "queries = [\n",
    "    \"What is the latest blog post about?\",\n",
    "    \"Can you summarize the latest blog post?\",\n",
    "    \"What are the key points from the latest blog post?\",\n",
    "    \"How does TheDataGuy standout in AI arena?\"\n",
    "    \"what is the dataguy all about?\"\n",
    "    \"whats his current job?\",\n",
    "    \"Tell me about ai Makerspace\",\n",
    "    \"Tell me about build ship and share\",\n",
    "    \"give me linkedin profile url\",\n",
    "    \"give me the latest blog post url\",\n",
    "    \"How many industries served by TheDataGuy?\",\n",
    "    \"What is relationship between TheDataGuy and AI Makerspace?\",\n",
    "    \"What is relationship between TheDataGuy and Sunrise Technologies?\",\n",
    "]\n",
    "\n",
    "replies = []\n",
    "for query in queries:\n",
    "    print(f\"Query: {query}\")\n",
    "    response = agent.invoke({\"messages\":[HumanMessage(content=query)]})\n",
    "    reply = get_message_text(response[\"messages\"][-1])\n",
    "    print(f\"Reply: {reply}\")\n",
    "    replies.append(reply)\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "\n",
    "# build a dataframe with queries and replies\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    \"Query\": queries,\n",
    "    \"Reply\": replies\n",
    "})\n",
    "# save the dataframe to a csv file\n",
    "df\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e508244",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f96d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = QdrantVectorStore.from_existing_collection(        \n",
    "        embedding=embedding_model, #type: ignore\n",
    "        collection_name=QDRANT_COLLECTION,\n",
    "        url=QDRANT_URL,\n",
    "        prefer_grpc=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e062c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.similarity_search(\"What is the best way to learn data science?\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7698ffd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d674b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from typing import cast\n",
    "from lets_talk.config import Configuration\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from lets_talk.utils import format_docs, get_message_text\n",
    "from pydantic  import BaseModel\n",
    "from langchain.chat_models import init_chat_model\n",
    "from lets_talk.state import InputState\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import (ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "\n",
    "\n",
    "thread_config = {\"thread_id\": \"thread-1\"}\n",
    "\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    \"\"\"Search the indexed documents for a query.\"\"\"\n",
    "\n",
    "    query: str\n",
    "\n",
    "\n",
    "async def generate_query(\n",
    "    state: InputState, *, config: RunnableConfig\n",
    ") -> dict[str, list[str]]:\n",
    "    \"\"\"Generate a search query based on the current state and configuration.\n",
    "\n",
    "    This function analyzes the messages in the state and generates an appropriate\n",
    "    search query. For the first message, it uses the user's input directly.\n",
    "    For subsequent messages, it uses a language model to generate a refined query.\n",
    "\n",
    "    Args:\n",
    "        state (State): The current state containing messages and other information.\n",
    "        config (RunnableConfig | None, optional): Configuration for the query generation process.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, list[str]]: A dictionary with a 'queries' key containing a list of generated queries.\n",
    "\n",
    "    Behavior:\n",
    "        - If there's only one message (first user input), it uses that as the query.\n",
    "        - For subsequent messages, it uses a language model to generate a refined query.\n",
    "        - The function uses the configuration to set up the prompt and model for query generation.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    \n",
    "    configuration = Configuration.from_runnable_config(config)\n",
    "\n",
    "    #get last HumanMessage from the messages\n",
    "    if not messages or not isinstance(messages[-1], HumanMessage):\n",
    "        raise ValueError(\"No HumanMessage found in the messages.\")\n",
    "    # Use the last HumanMessage as the user query\n",
    "    if len(messages) == 1:\n",
    "        # If it's the first message, use it directly as the query\n",
    "        user_query = get_message_text(messages[0])\n",
    "    else:\n",
    "        # For subsequent messages, use the last HumanMessage\n",
    "        if not isinstance(messages[-1], HumanMessage):\n",
    "            raise ValueError(\"Last message is not a HumanMessage.\")\n",
    "        # Use the last HumanMessage as the user query\n",
    "\n",
    "    user_query = get_message_text(messages[-1]) if messages else \"\"\n",
    "\n",
    "    docs = retriever.invoke(user_query,config=config)  # type: ignore\n",
    "\n",
    "    docs_str = format_docs(docs)\n",
    "\n",
    "    # Create Tool message from doc_str\n",
    "    tool_message = AIMessage(\n",
    "        content=f\"Here are the relevant documents:\\n{docs_str}\",\n",
    "        additional_kwargs={\"tool_calls\": []},\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    # Feel free to customize the prompt, model, and other logic!\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            SystemMessagePromptTemplate.from_template(configuration.query_system_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ]\n",
    "    )\n",
    "    model = init_chat_model(configuration.query_model,temperature=0).with_structured_output(\n",
    "        SearchQuery\n",
    "    )\n",
    "\n",
    "    message_value = {\n",
    "            \"messages\":[*messages, tool_message],\n",
    "            \"queries\": \"\\n- \".join(state.get(\"queries\", [])),\n",
    "            \"system_time\": datetime.datetime.now(tz=datetime.timezone.utc).isoformat(),\n",
    "        }\n",
    "    \n",
    "    chain = prompt | model\n",
    "\n",
    "    generated = cast(SearchQuery, await chain.ainvoke(message_value, config))\n",
    "    return {\n",
    "        \"queries\": [generated.query],\n",
    "    }\n",
    "\n",
    "\n",
    "result = await generate_query(\n",
    "    InputState(\n",
    "        messages=[\n",
    "            HumanMessage(\n",
    "                content=\"How does TheDataGuy standout in AI arena?\",\n",
    "            ),\n",
    "\n",
    "        ],\n",
    "\n",
    "    ),\n",
    "    config=RunnableConfig(config=thread_config), # type: ignore\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4365f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(result['queries'][0],config=RunnableConfig(config=thread_config))  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51a76f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_1 = await generate_query(InputState(\n",
    "        messages=[\n",
    "            HumanMessage(\n",
    "                content=\"Give me a rundown in number about his career and blog\",\n",
    "            ),\n",
    "        ],\n",
    "        queries=result[\"queries\"],  # type: ignore\n",
    "    ),\n",
    "    config=RunnableConfig(config=thread_config), # type: ignore\n",
    ")\n",
    "print(result_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54205fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(result_1['queries'][0],config=RunnableConfig(config=thread_config))  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a60aa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"Profession experience TheDataGuy\",config=RunnableConfig(config=thread_config))  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4832ea91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370f45c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57ea7b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b7567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.invoke({\"messages\":[HumanMessage(content=\"How does TheDataGuy standout in AI arena?\")]},config=RunnableConfig(config=thread_config)) # type: ignore\n",
    "print(get_message_text(response['messages'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c7f3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.invoke({\"messages\":[AIMessage(content=\"User is currently visiting `https://thedataguy.pro/` page\"),HumanMessage(content=\"How does TheDataGuy standout in AI arena?\")]},config=RunnableConfig(config=thread_config)) # type: ignore\n",
    "\n",
    "print(get_message_text(response['messages'][-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a772714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage \n",
    "response = agent.invoke({\"messages\":[ToolMessage(content=\"User is currently visiting https://thedataguy.pro/about/\", tool_call_id=\"current_visiting_page\"), HumanMessage(content=\"Give me a rundown in number about his career and blog?\")]},config=RunnableConfig(config=thread_config)) # type: ignore\n",
    "\n",
    "print(get_message_text(response['messages'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3106dbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage \n",
    "response = agent.invoke({\"messages\": [HumanMessage(content=\"What is TheDataGuy currently employeed?\"), ToolMessage(content=\"User is currently visiting https://thedataguy.pro/about/\", tool_call_id=\"current_visiting_page\")]},config=RunnableConfig(config=thread_config)) # type: ignore\n",
    "\n",
    "print(get_message_text(response['messages'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a3f558",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.invoke(\"What is relationship between Sunrise Technologies and TheDataGuy?\",config=RunnableConfig(config=thread_config))  # type: ignore\n",
    "docs_str = format_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f18517",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeed307",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.invoke({\"messages\": [HumanMessage(content=\"Where is TheDataGuy currently working?\")]},config=RunnableConfig(config=thread_config)) # type: ignore\n",
    "print(get_message_text(response['messages'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15210a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.invoke({\"messages\": [HumanMessage(content=\"What is relationship between Sunrise Technologies and TheDataGuy?\")]},config=RunnableConfig(config=thread_config)) # type: ignore\n",
    "\n",
    "print(get_message_text(response['messages'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532fb425",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2 = await generate_query(InputState(\n",
    "        messages=[\n",
    "            HumanMessage(\n",
    "                content=\"Give me his X handle\",\n",
    "            ),\n",
    "        ],\n",
    "        queries=[*result[\"queries\"],*result_1['queries']],  # type: ignore\n",
    "    ),\n",
    "    config=RunnableConfig(config=thread_config), # type: ignore\n",
    ")\n",
    "print(result_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6430ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(result_2['queries'][0],config=RunnableConfig(config=thread_config))  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e33fca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_3 = await generate_query(InputState(\n",
    "        messages=[\n",
    "            HumanMessage(\n",
    "                content=\"And youtube channel\",\n",
    "            ),\n",
    "        ],\n",
    "        queries=[*result[\"queries\"],*result_1['queries'],*result_2['queries']],  # type: ignore\n",
    "    ),\n",
    "    config=RunnableConfig(config=thread_config), # type: ignore\n",
    ")\n",
    "print(result_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632ea787",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(result_3['queries'][0],config=RunnableConfig(config=thread_config))  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85e443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"Give me a rundown in number about his career and blog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3b2dca",
   "metadata": {},
   "source": [
    "## Testing the Vector Store\n",
    "\n",
    "Let's test the vector store with a few queries to make sure it's working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b552e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever from the vector store\n",
    "\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"Give me projects list?\",\n",
    "    \"What is RAGAS?\",\n",
    "    \"How to build research agents?\",\n",
    "    \"What is metric driven development?\",\n",
    "    \"Who is TheDataGuy?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    docs = retriever.invoke(query)\n",
    "    print(f\"Retrieved {len(docs)} documents:\")\n",
    "    for i, doc in enumerate(docs):\n",
    "        title = doc.metadata.get(\"post_title\", \"Unknown\")\n",
    "        url = doc.metadata.get(\"url\", \"No URL\")\n",
    "        print(f\"{i+1}. {title} ({url})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdd6899",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
