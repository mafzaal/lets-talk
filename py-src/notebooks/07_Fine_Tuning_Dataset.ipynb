{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c95ab233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding package root to sys.path: /home/mafzaal/source/lets-talk/py-src\n",
      "Current notebook directory: /home/mafzaal/source/lets-talk/py-src/notebooks\n",
      "Project root: /home/mafzaal/source/lets-talk\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root to the Python path\n",
    "package_root = os.path.abspath(os.path.join(os.getcwd(), \"../\"))\n",
    "print(f\"Adding package root to sys.path: {package_root}\")\n",
    "if package_root not in sys.path:\n",
    "\tsys.path.append(package_root)\n",
    "\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "print(f\"Current notebook directory: {notebook_dir}\")\n",
    "# change to the directory to the root of the project\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "print(f\"Project root: {project_root}\")\n",
    "os.chdir(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15e97530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4f2ddc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lets_talk.utils.blog as blog\n",
    "import lets_talk.utils.eval as eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "123779af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 3411.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 14 documents from data/\n",
      "Split 14 documents into 162 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "docs = blog.load_blog_posts()\n",
    "docs = blog.update_document_metadata(docs)\n",
    "split_docs = blog.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b742838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "qa_chat_model = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "qa_prompt = \"\"\"\\\n",
    "Given the following context, you must generate questions based on only the provided context.\n",
    "You are to generate {n_questions} questions which should be provided in the following format:\n",
    "\n",
    "1. QUESTION #1\n",
    "2. QUESTION #2\n",
    "...\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "qa_prompt_template = ChatPromptTemplate.from_template(qa_prompt)\n",
    "question_generation_chain = qa_prompt_template | qa_chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5488c3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import asyncio\n",
    "\n",
    "\n",
    "def extract_questions(response_text,n_questions):\n",
    "    # Split the response text into lines\n",
    "    lines = response_text.strip().split('\\n')\n",
    "\n",
    "    # Extract questions (format: \"1. QUESTION\")\n",
    "    extracted_questions = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line and any(line.startswith(f\"{i}.\") for i in range(1, n_questions+1)):\n",
    "            # Remove the number prefix and get just the question\n",
    "            question = line.split('.', 1)[1].strip()\n",
    "            extracted_questions.append(question)\n",
    "\n",
    "    return extracted_questions\n",
    "\n",
    "async def create_questions(documents, n_questions, chain):\n",
    "    question_set = []\n",
    "    \n",
    "    for doc in tqdm.tqdm(documents):\n",
    "        \n",
    "        context = doc.page_content\n",
    "\n",
    "        # Generate questions using the question generation chain\n",
    "        response = await chain.ainvoke({\n",
    "            \"context\": context,\n",
    "            \"n_questions\": n_questions\n",
    "        })\n",
    "\n",
    "        questions = extract_questions(response.content,n_questions)\n",
    "        \n",
    "        for i, question in enumerate(questions):\n",
    "            questions.append({\"question\":question, \"context\": context})\n",
    "    return question_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adb3ae7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted questions:\n",
      "1.  What is the primary purpose of the Ragas evaluation framework in LLM applications?\n",
      "2.  Why is it important to have reliable metrics when assessing the performance of LLM-based systems?\n",
      "3.  What types of applications can benefit from using the Ragas framework for evaluation?\n"
     ]
    }
   ],
   "source": [
    "context = split_docs[0].page_content\n",
    "n_questions = 3\n",
    "response = question_generation_chain.invoke({\"context\": context, \"n_questions\": n_questions})\n",
    "questions = extract_questions(response.content, n_questions)\n",
    "print(\"Extracted questions:\")\n",
    "for i, question in enumerate(questions):\n",
    "    print(f\"{i + 1}.  {question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4a75f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = split_docs[:2]\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1ece53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "ds = await create_questions(documents=docs, n_questions=3, chain=question_generation_chain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
