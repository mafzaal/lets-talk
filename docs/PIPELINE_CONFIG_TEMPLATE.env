# Pipeline Configuration Template
# Copy this file to .env in your project root and customize the values

# =============================================================================
# REQUIRED CONFIGURATION
# =============================================================================

# Data source directory containing your blog posts
DATA_DIR=data/

# Vector database storage path
VECTOR_STORAGE_PATH=db/vector_store

# Qdrant collection name for storing embeddings
QDRANT_COLLECTION=blog_documents

# Embedding model to use (examples below)
# For Ollama (local):
EMBEDDING_MODEL=ollama:snowflake-arctic-embed2:latest
# For OpenAI:
# EMBEDDING_MODEL=openai:text-embedding-3-small
# For Hugging Face:
# EMBEDDING_MODEL=huggingface:sentence-transformers/all-MiniLM-L6-v2

# =============================================================================
# DATA PROCESSING CONFIGURATION
# =============================================================================

# File pattern for blog posts (glob pattern)
DATA_DIR_PATTERN=*.md

# Base URLs for generating links
BASE_URL=https://yourblog.com
BLOG_BASE_URL=https://yourblog.com/blog

# Only index published posts (if your posts have published: true/false)
INDEX_ONLY_PUBLISHED_POSTS=True

# RSS feed URL (if applicable)
RSS_URL=https://yourblog.com/rss.xml

# =============================================================================
# OUTPUT CONFIGURATION
# =============================================================================

# Directory for pipeline output and statistics
OUTPUT_DIR=output/
STATS_OUTPUT_DIR=./stats

# Custom output filenames (optional)
BLOG_STATS_FILENAME=blog_stats_latest.json
BLOG_DOCS_FILENAME=blog_docs.csv
HEALTH_REPORT_FILENAME=health_report.json
CI_SUMMARY_FILENAME=ci_summary.json
BUILD_INFO_FILENAME=vector_store_build_info.json
DEFAULT_METADATA_CSV_FILENAME=blog_metadata.csv

# =============================================================================
# DOCUMENT PROCESSING OPTIONS
# =============================================================================

# Enable document chunking for better retrieval
USE_CHUNKING=True

# Chunking strategy: "semantic" or "text_splitter"
CHUNKING_STRATEGY=semantic

# Chunk size in characters (if using text_splitter)
CHUNK_SIZE=1000

# Overlap between chunks in characters
CHUNK_OVERLAP=200

# Parent document retrieval settings
PARENT_DOCUMENT_RETRIEVAL=True
PARENT_DOCUMENT_CHILD_CHUNK_SIZE=200

# =============================================================================
# RETRIEVAL CONFIGURATION
# =============================================================================

# Enable BM25 retrieval for hybrid search
BM25_RETRIEVAL=True

# Enable multi-query retrieval
MULTI_QUERY_RETRIEVAL=True

# Maximum search results to return
MAX_SEARCH_RESULTS=4

# =============================================================================
# INCREMENTAL INDEXING CONFIGURATION
# =============================================================================

# Incremental indexing mode: "auto", "incremental", "full"
INCREMENTAL_MODE=auto

# Checksum algorithm for change detection: "sha256", "md5"
CHECKSUM_ALGORITHM=sha256

# Auto-detect document changes
AUTO_DETECT_CHANGES=True

# Threshold for falling back to full rebuild (0.0-1.0)
# If more than this percentage of documents changed, do full rebuild
INCREMENTAL_FALLBACK_THRESHOLD=0.8

# Default timestamp for new documents
DEFAULT_INDEXED_TIMESTAMP=0.0

# =============================================================================
# PERFORMANCE OPTIMIZATION
# =============================================================================

# Number of documents to process per batch
BATCH_SIZE=50

# Enable batch processing optimizations
ENABLE_BATCH_PROCESSING=True

# Enable performance monitoring
ENABLE_PERFORMANCE_MONITORING=True

# Enable adaptive chunking
ADAPTIVE_CHUNKING=True

# Maximum number of backup files to keep
MAX_BACKUP_FILES=3

# Pause between batches (seconds)
BATCH_PAUSE_SECONDS=0.1

# Maximum concurrent operations
MAX_CONCURRENT_OPERATIONS=5

# =============================================================================
# VECTOR DATABASE CONFIGURATION
# =============================================================================

# Force recreation of vector store
FORCE_RECREATE=False

# Save document statistics
SHOULD_SAVE_STATS=True

# Create vector database
CREATE_VECTOR_DB=True

# Qdrant URL (leave empty for local storage)
QDRANT_URL=

# =============================================================================
# LLM CONFIGURATION
# =============================================================================

# Primary LLM model for the agent
LLM_MODEL=openai:gpt-4o-mini

# LLM temperature (0.0-1.0)
LLM_TEMPERATURE=0.1

# LLM models for synthetic data generation and evaluation
SDG_LLM_MODEL=openai:gpt-4.1
EVAL_LLM_MODEL=openai:gpt-4.1



# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Log format
LOG_FORMAT=%(asctime)s - %(name)s - %(levelname)s - %(message)s

# Logger name
LOGGER_NAME=blog-pipeline

# =============================================================================
# EXAMPLES FOR DIFFERENT ENVIRONMENTS
# =============================================================================

# Development Environment
# ------------------------
# LOG_LEVEL=DEBUG
# BATCH_SIZE=25
# INCREMENTAL_MODE=auto
# STATS_OUTPUT_DIR=./dev-stats

# Production Environment
# -----------------------
# LOG_LEVEL=WARNING
# BATCH_SIZE=100
# MAX_CONCURRENT_OPERATIONS=10
# STATS_OUTPUT_DIR=/var/log/blog-pipeline

# CI/CD Environment
# -----------------
# LOG_LEVEL=INFO
# FORCE_RECREATE=False
# INCREMENTAL_MODE=auto
# STATS_OUTPUT_DIR=./artifacts

# =============================================================================
# EXAMPLE EMBEDDING MODELS
# =============================================================================

# Local Models (Ollama)
# ---------------------
# EMBEDDING_MODEL=ollama:snowflake-arctic-embed2:latest
# EMBEDDING_MODEL=ollama:nomic-embed-text:latest
# EMBEDDING_MODEL=ollama:all-minilm:latest

# OpenAI Models
# -------------
# EMBEDDING_MODEL=openai:text-embedding-3-small
# EMBEDDING_MODEL=openai:text-embedding-3-large
# EMBEDDING_MODEL=openai:text-embedding-ada-002

# Hugging Face Models
# -------------------
# EMBEDDING_MODEL=huggingface:sentence-transformers/all-MiniLM-L6-v2
# EMBEDDING_MODEL=huggingface:sentence-transformers/all-mpnet-base-v2
# EMBEDDING_MODEL=huggingface:BAAI/bge-small-en-v1.5

# =============================================================================
# TROUBLESHOOTING SETTINGS
# =============================================================================

# If you encounter memory issues:
# BATCH_SIZE=10
# CHUNK_SIZE=500
# MAX_CONCURRENT_OPERATIONS=2

# If incremental indexing isn't working:
# INCREMENTAL_MODE=full
# CHECKSUM_ALGORITHM=sha256
# AUTO_DETECT_CHANGES=True

# If processing is slow:
# BATCH_SIZE=100
# MAX_CONCURRENT_OPERATIONS=8
# ENABLE_BATCH_PROCESSING=True
